{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/JavierCIPFPD/Big-Data-Aplicado/blob/main/0111_Ejemplo_Llama-2-13b-chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"cqHFjrXVBJcn"},"source":["# Llama 2\n","\n","\n","Meta, en colaboración con Microsoft, lanzó Lama 2 con un montón de nuevos modelos lingüísticos de código abierto.\n","\n","Llama 2 se ha entrenado 40% más de datos que su predecesor predecesor, llama 1, y se ha duplicado del contexto se ha duplicado.\n","\n","Llama 2 viene en tres diferentes tamaños un modelo de 7 mil millones, un modelo de 13 mil millones de parámetros y un modelo de 70 mil millones y además todos han sido pre-entrenados en la asombrosa cantidad de 2 trillones de tokens con una longitud de contexto de 4.000 tokens, lo que significa que estos modelos están sólo un paso por detrás de Chat GPT.\n","\n","Aquí tienes su web de descarga: https://ai.meta.com/llama/\n","\n","Ahora lo vamos a ejecutar en local en nuestra máquina de colab, es super sencillo:\n","\n","**NOTA: tardará unos 5 min en estar listo.**\n","\n","Cuando esté listo haz clic en el enlace que aparece abajo del todo, será algo parecido a:\n","\n","Running on public URL: https://1c283dc96ce73a71d8.gradio.live\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"VCFOzsQSHbjM","outputId":"e8269057-ec95-4744-90c5-b72e8590b4f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","Selecting previously unselected package libc-ares2:amd64.\n","(Reading database ... 123622 files and directories currently installed.)\n","Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n","Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n","Selecting previously unselected package libaria2-0:amd64.\n","Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n","Unpacking libaria2-0:amd64 (1.36.0-1) ...\n","Selecting previously unselected package aria2.\n","Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n","Unpacking aria2 (1.36.0-1) ...\n","Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n","Setting up libaria2-0:amd64 (1.36.0-1) ...\n","Setting up aria2 (1.36.0-1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Cloning into 'text-generation-webui'...\n","remote: Enumerating objects: 17379, done.\u001b[K\n","remote: Total 17379 (delta 0), reused 0 (delta 0), pack-reused 17379 (from 1)\u001b[K\n","Receiving objects: 100% (17379/17379), 26.44 MiB | 19.84 MiB/s, done.\n","Resolving deltas: 100% (12281/12281), done.\n","/content/text-generation-webui\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.2/355.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.9/732.9 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/152.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.24.0 which is incompatible.\n","albumentations 1.4.15 requires pydantic>=2.7.0, but you have pydantic 1.10.18 which is incompatible.\n","chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.24.0 which is incompatible.\n","seaborn 0.13.2 requires numpy!=1.24.0,>=1.20, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","5c287f|\u001b[1;32mOK\u001b[0m  |    65KiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","f76dd6|\u001b[1;32mOK\u001b[0m  |    18KiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/generation_config.json\n","\n","Status Legend:\n","(OK):download completed.\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","ca0ae9|\u001b[1;32mOK\u001b[0m  |   174MiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/model-00001-of-00002.safetensors\n","\n","Status Legend:\n","(OK):download completed.\n","\u001b[0m\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","a7f5d5|\u001b[1;32mOK\u001b[0m  |   173MiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/model-00002-of-00002.safetensors\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","792649|\u001b[1;32mOK\u001b[0m  |   3.2MiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/model.safetensors.index.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","fea587|\u001b[1;32mOK\u001b[0m  |    44KiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/special_tokens_map.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","57e985|\u001b[1;32mOK\u001b[0m  |   6.7MiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/tokenizer.json\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","e8aa51|\u001b[1;32mOK\u001b[0m  |   9.1MiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/tokenizer.model\n","\n","Status Legend:\n","(OK):download completed.\n","\n","Download Results:\n","gid   |stat|avg speed  |path/URI\n","======+====+===========+=======================================================\n","ef4960|\u001b[1;32mOK\u001b[0m  |   114KiB/s|/content/text-generation-webui/models/code-llama-instruct-7b/tokenizer_config.json\n","\n","Status Legend:\n","(OK):download completed.\n","/content/text-generation-webui\n","2024-10-23 10:53:17 WARNING:\u001b[33mThe gradio \"share link\" feature uses a proprietary executable to create a reverse tunnel. Use it with care.\u001b[0m\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n","  torch.utils._pytree._register_pytree_node(\n","2024-10-23 10:53:26.866664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-23 10:53:27.112808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-23 10:53:27.182086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-23 10:53:29.180462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n","  torch.utils._pytree._register_pytree_node(\n","2024-10-23 10:53:33 INFO:\u001b[32mLoading settings from /content/settings.yaml...\u001b[0m\n","2024-10-23 10:53:33 INFO:\u001b[32mLoading /content/text-generation-webui/models/code-llama-instruct-7b...\u001b[0m\n","Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.50it/s]\n","2024-10-23 10:54:37 WARNING:\u001b[33mmodels/code-llama-instruct-7b/tokenizer_config.json is different from the original LlamaTokenizer file. It is either customized or outdated.\u001b[0m\n","2024-10-23 10:54:37 INFO:\u001b[32mLoaded the model in 63.83 seconds.\n","\u001b[0m\n","2024-10-23 10:54:37 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n","Running on local URL:  http://127.0.0.1:7860\n","Running on public URL: https://06d40e292178b8c053.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","Output generated in 4.94 seconds (2.43 tokens/s, 12 tokens, context 67, seed 963692721)\n","Output generated in 2.51 seconds (11.14 tokens/s, 28 tokens, context 89, seed 1120107215)\n","Output generated in 2.62 seconds (9.56 tokens/s, 25 tokens, context 122, seed 2132377352)\n","Output generated in 15.31 seconds (13.00 tokens/s, 199 tokens, context 161, seed 740908075)\n","Output generated in 5.54 seconds (10.64 tokens/s, 59 tokens, context 371, seed 94326296)\n","Output generated in 16.68 seconds (11.93 tokens/s, 199 tokens, context 446, seed 1158843333)\n","Output generated in 3.32 seconds (7.53 tokens/s, 25 tokens, context 667, seed 1523327622)\n","Output generated in 3.01 seconds (5.97 tokens/s, 18 tokens, context 706, seed 1478151885)\n"]}],"source":["%cd /content\n","!apt-get -y install -qq aria2\n","\n","!git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n","%cd /content/text-generation-webui\n","!pip install -q -r requirements.txt\n","\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/generation_config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o generation_config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/model-00001-of-00002.safetensors -d /content/text-generation-webui/models/code-llama-instruct-7b -o model-00001-of-00002.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/model-00002-of-00002.safetensors -d /content/text-generation-webui/models/code-llama-instruct-7b -o model-00002-of-00002.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/model.safetensors.index.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o model.safetensors.index.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o special_tokens_map.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/tokenizer.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/tokenizer.model -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer.model\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer_config.json\n","\n","!echo \"dark_theme: true\" > /content/settings.yaml\n","!echo \"chat_style: wpp\" >> /content/settings.yaml\n","\n","%cd /content/text-generation-webui\n","!python server.py --share --settings /content/settings.yaml --model /content/text-generation-webui/models/code-llama-instruct-7b"]},{"cell_type":"markdown","source":["## Cambiar el directorio a /content:\n","\n","%cd /content\n","\n","Esto establece el directorio actual en /content de Colab.\n","\n","## Instalar aria2:\n","\n","!apt-get -y install -qq aria2\n","\n","aria2 es un programa de descarga que permite acelerar la descarga de archivos. Es especialmente útil en Colab para manejar archivos grandes desde varios servidores.\n","\n","## Clonar el repositorio de text-generation-webui:\n","\n","\n","!git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n","\n","%cd /content/text-generation-webui\n","\n","Este comando clona la versión v2.5 del repositorio text-generation-webui, que contiene una interfaz para manejar modelos de generación de texto.\n","\n","## Instalar dependencias:\n","\n","\n","!pip install -q -r requirements.txt\n","\n","Instala las dependencias listadas en requirements.txt, necesarias para ejecutar el entorno y el servidor web de text-generation-webui.\n","\n","## Descargar los archivos de modelo desde Hugging Face usando aria2:\n","Cada comando aria2c descarga archivos específicos del modelo CodeLlama-7B-Instruct y los guarda en la carpeta de modelos:\n","\n","\n","\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M [URL] -d /content/text-generation-webui/models/code-llama-instruct-7b -o [filename]\n","\n","\n","Aquí, -c -x 16 -s 16 -k 1M optimiza la descarga. Los archivos descargados incluyen: config.json, generation_config.json, archivos de pesos (model-00001-of-00002.safetensors y model-00002-of-00002.safetensors), model.safetensors.index.json, y archivos del tokenizador (special_tokens_map.json, tokenizer.json, tokenizer.model, tokenizer_config.json).\n","\n","## Configurar la interfaz:\n","\n","\n","\n","!echo \"dark_theme: true\" > /content/settings.yaml\n","!echo \"chat_style: wpp\" >> /content/settings.yaml\n","Estas líneas crean un archivo settings.yaml en /content y configuran la interfaz para tener un tema oscuro y un estilo de chat tipo WhatsApp.\n","\n","## Iniciar el servidor web:\n","\n","\n","%cd /content/text-generation-webui\n","!python server.py --share --settings /content/settings.yaml --model /content/text-generation-webui/models/code-llama-instruct-7b\n","\n","Este comando inicia el servidor de text-generation-webui, haciendo que esté disponible una interfaz web pública y usando el modelo code-llama-instruct-7b con la configuración especificada."],"metadata":{"id":"anryLxUeTH2J"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}