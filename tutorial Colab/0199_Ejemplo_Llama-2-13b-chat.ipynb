{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/JavierCIPFPD/Big-Data-Aplicado/blob/main/0111_Ejemplo_Llama-2-13b-chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"cqHFjrXVBJcn"},"source":["# Llama 2\n","\n","\n","Meta, en colaboración con Microsoft, lanzó Lama 2 con un montón de nuevos modelos lingüísticos de código abierto.\n","\n","Llama 2 se ha entrenado 40% más de datos que su predecesor predecesor, llama 1, y se ha duplicado del contexto se ha duplicado.\n","\n","Llama 2 viene en tres diferentes tamaños un modelo de 7 mil millones, un modelo de 13 mil millones de parámetros y un modelo de 70 mil millones y además todos han sido pre-entrenados en la asombrosa cantidad de 2 trillones de tokens con una longitud de contexto de 4.000 tokens, lo que significa que estos modelos están sólo un paso por detrás de Chat GPT.\n","\n","Aquí tienes su web de descarga: https://ai.meta.com/llama/\n","\n","Ahora lo vamos a ejecutar en local en nuestra máquina de colab, es super sencillo:\n","\n","**NOTA: tardará unos 5 min en estar listo.**\n","\n","Cuando esté listo haz clic en el enlace que aparece abajo del todo, será algo parecido a:\n","\n","Running on public URL: https://1c283dc96ce73a71d8.gradio.live\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCFOzsQSHbjM","outputId":"3cb0066a-2625-4443-e729-5d6dd75a9615"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","fatal: destination path 'text-generation-webui' already exists and is not an empty directory.\n","/content/text-generation-webui\n","\u001b[31mERROR: auto_gptq-0.4.2+cu117-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["%cd /content\n","!apt-get -y install -qq aria2\n","\n","!git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n","%cd /content/text-generation-webui\n","!pip install -q -r requirements.txt\n","\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/generation_config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o generation_config.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/model-00001-of-00002.safetensors -d /content/text-generation-webui/models/code-llama-instruct-7b -o model-00001-of-00002.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/model-00002-of-00002.safetensors -d /content/text-generation-webui/models/code-llama-instruct-7b -o model-00002-of-00002.safetensors\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/model.safetensors.index.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o model.safetensors.index.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/special_tokens_map.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o special_tokens_map.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/tokenizer.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer.json\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/resolve/main/tokenizer.model -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer.model\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16/raw/main/tokenizer_config.json -d /content/text-generation-webui/models/code-llama-instruct-7b -o tokenizer_config.json\n","\n","!echo \"dark_theme: true\" > /content/settings.yaml\n","!echo \"chat_style: wpp\" >> /content/settings.yaml\n","\n","%cd /content/text-generation-webui\n","!python server.py --share --settings /content/settings.yaml --model /content/text-generation-webui/models/code-llama-instruct-7b"]},{"cell_type":"markdown","source":["## Cambiar el directorio a /content:\n","\n","%cd /content\n","\n","Esto establece el directorio actual en /content de Colab.\n","\n","## Instalar aria2:\n","\n","!apt-get -y install -qq aria2\n","\n","aria2 es un programa de descarga que permite acelerar la descarga de archivos. Es especialmente útil en Colab para manejar archivos grandes desde varios servidores.\n","\n","## Clonar el repositorio de text-generation-webui:\n","\n","\n","!git clone -b v2.5 https://github.com/camenduru/text-generation-webui\n","\n","%cd /content/text-generation-webui\n","\n","Este comando clona la versión v2.5 del repositorio text-generation-webui, que contiene una interfaz para manejar modelos de generación de texto.\n","\n","## Instalar dependencias:\n","\n","\n","!pip install -q -r requirements.txt\n","\n","Instala las dependencias listadas en requirements.txt, necesarias para ejecutar el entorno y el servidor web de text-generation-webui.\n","\n","## Descargar los archivos de modelo desde Hugging Face usando aria2:\n","Cada comando aria2c descarga archivos específicos del modelo CodeLlama-7B-Instruct y los guarda en la carpeta de modelos:\n","\n","\n","\n","!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M [URL] -d /content/text-generation-webui/models/code-llama-instruct-7b -o [filename]\n","\n","\n","Aquí, -c -x 16 -s 16 -k 1M optimiza la descarga. Los archivos descargados incluyen: config.json, generation_config.json, archivos de pesos (model-00001-of-00002.safetensors y model-00002-of-00002.safetensors), model.safetensors.index.json, y archivos del tokenizador (special_tokens_map.json, tokenizer.json, tokenizer.model, tokenizer_config.json).\n","\n","## Configurar la interfaz:\n","\n","\n","\n","!echo \"dark_theme: true\" > /content/settings.yaml\n","!echo \"chat_style: wpp\" >> /content/settings.yaml\n","Estas líneas crean un archivo settings.yaml en /content y configuran la interfaz para tener un tema oscuro y un estilo de chat tipo WhatsApp.\n","\n","## Iniciar el servidor web:\n","\n","\n","%cd /content/text-generation-webui\n","!python server.py --share --settings /content/settings.yaml --model /content/text-generation-webui/models/code-llama-instruct-7b\n","\n","Este comando inicia el servidor de text-generation-webui, haciendo que esté disponible una interfaz web pública y usando el modelo code-llama-instruct-7b con la configuración especificada."],"metadata":{"id":"anryLxUeTH2J"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}